{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HRiNtQlSQEbT"
   },
   "source": [
    "## Overview and Abstract\n",
    "**MULTILABEL CLASSIFICATION PROBLEM**\n",
    "\n",
    "This project consists of creating a MutliLabel classifier that predicts the category associated to a text-based search query. Nine categories are concatenated as Label. Data was processed using word-tokenization ('title') to conduct Natural Language Processing. Further, Labels were transformed into nine binary vectors which eased learning. Random Forest was again selected as the baseline, as tokenized words created a highly dimensional space. The chosen MultiClass methods were a dense, deep NN; a LSTM; and a CNN. These were evaluated using F1 scores. Differences between models used was negligible. Alternative embedding techniques and additional features would likely improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "id": "4wPdQuBPP_CK"
   },
   "outputs": [],
   "source": [
    "### PACKAGES ###\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Packages used for Feature Engineering\n",
    "  # General\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "  # A: Correcting class imbalances\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler  \n",
    "  # B: Tokenizing words\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "# Packages used for Shallow ML (scikit-learn)\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier # Shallow method deployed\n",
    "# Packages used for Deep ML (keras)\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras import backend as K\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Mcb9vqEVQ8c"
   },
   "source": [
    "## NLP project: Method\n",
    "\n",
    "Some features of the dataset were altered to improve its appropriacy for the chosen classification models. The feature processing is as follows:\n",
    "- Cleaning text by removing punctuation, capitalization, 'stopwords', and other characters of minimal value.\n",
    "- Tokenization and padding were carried out to make sure the dimensions were fixed in the input.\n",
    "- One-hot encoding target labels (y) to transform MultiLabel inputs to Multiclass. This simplifies inferences made.\n",
    "- A validation set was created.\n",
    "\n",
    "A Random Forest was chosen as a baseline model for subsequent neural networks. A Random Forest is beneficial because text padding creates a highly dimensional space. Yet, the independence of decisions trees within a Random Forest will capture the relationships between other dimensions / words.\n",
    "\n",
    "As Multiclass  classifiers, each neural network outputs 9 neurons using a sigmoid activation function. Optimal hyperparameters for each classifier were found using Grid Search. Furthermore, the loss function for all classifiers was set to ‘binary_crossentropy’. All other activation functions were set to ‘relu’ with a ‘he-normal’ initializer.\n",
    "- The 4-layer dense, deep neural network attempts to capture complexities present in tokenized 'title' data exclusively. Optimized F1 score was found when epochs were set to 10 and batch size to 400.\n",
    "- A Recurrent Neural Network with Long-Short-Term Memory (**Hochreiter, S. & Schmidhuber, J., 1997**) attempts to capture immediate and longer-term representations, suitable for sentences of text.\n",
    "- Though primarily deployed for image classification problems, more recently  Convolutional Neural Networks may be used for NLP, to see whether Convolutional Layers can identify spatial dependence and order between specific words (**Kalchbrenner, N. et al., 2016**).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "id": "oHqbWROhVwmd"
   },
   "outputs": [],
   "source": [
    "## ARCHITECTURE ##\n",
    "# Dense, deep neural network\n",
    "class Model_Multiclass_Dense(keras.Model):\n",
    "  def __init__(self, input_dim, input_length, output_dim = 264, activation1 = 'relu', activation2 = 'relu', kernel_initializer1 = 'he-normal', **kwargs):\n",
    "    super(Model_Multiclass_Dense, self).__init__(**kwargs)\n",
    "    self.embedding = layers.Embedding(input_dim = input_dim, output_dim = output_dim, input_length = input_length)\n",
    "    self.flatten = layers.Flatten()\n",
    "    self.hidden1 = layers.Dense(units = 50)\n",
    "    self.hidden2 = layers.Dense(units = 50)\n",
    "    self.main_output = layers.Dense(units = 9, activation = 'sigmoid')\n",
    "  def call(self, inputs):\n",
    "    embedding = self.embedding(inputs)\n",
    "    flatten = self.flatten(embedding)\n",
    "    hidden1 = self.hidden1(flatten)\n",
    "    hidden2 = self.hidden2(hidden1)\n",
    "    main_output = self.main_output(hidden2)\n",
    "    return main_output\n",
    "# LSTM\n",
    "class Model_Multiclass_LSTM(keras.Model):\n",
    "  def __init__(self, input_dim, input_length, output_dim = 100, activation1 = 'relu', activation2 = 'relu', kernel_initializer1 = 'he-normal', kernel_initializer2 = 'he-normal', **kwargs):\n",
    "    super(Model_Multiclass_LSTM, self).__init__(**kwargs)\n",
    "    self.embedding = layers.Embedding(input_dim = input_dim, output_dim = output_dim, input_length = input_length)\n",
    "    self.LSTM = LSTM(128)\n",
    "    self.main_output = layers.Dense(units = 9, activation = 'sigmoid')\n",
    "  def call(self, inputs):\n",
    "    embedding = self.embedding(inputs)\n",
    "    LSTM = self.LSTM(embedding)\n",
    "    main_output = self.main_output(LSTM)\n",
    "    return main_output\n",
    "# CNN\n",
    "class Model_Multiclass_CNN(keras.Model):\n",
    "  def __init__(self, input_dim, input_length, output_dim = 100, activation1 = 'relu', activation2 = 'relu', kernel_initializer1 = 'he-normal', kernel_initializer2 = 'he-normal', **kwargs):\n",
    "    super(Model_Multiclass_CNN, self).__init__(**kwargs)\n",
    "    self.embedding = layers.Embedding(input_dim = input_dim, output_dim = output_dim, input_length = input_length)\n",
    "    self.conv = layers.Conv1D(filters=32, kernel_size=8, activation='relu')\n",
    "    self.pooling = layers.MaxPooling1D(pool_size=2)\n",
    "    self.flatten = layers.Flatten()\n",
    "    self.hidden = layers.Dense(units = 10, activation='relu')\n",
    "    self.main_output = layers.Dense(units = 9, activation = 'sigmoid')\n",
    "  def call(self, inputs):\n",
    "    embedding = self.embedding(inputs)\n",
    "    conv = self.conv(embedding)\n",
    "    pooling = self.pooling(conv)\n",
    "    flatten = self.flatten(pooling)\n",
    "    hidden = self.hidden(flatten)\n",
    "    main_output = self.main_output(hidden)\n",
    "    return main_output\n",
    "## TRAINING FUNCTIONS ##\n",
    "# Dense, deep neural network, LSTM and CNN\n",
    "def ModelTrainerMulti(X, y, batch_size, epochs, model, input_dim, input_length):\n",
    "  model = model(input_dim = input_dim, input_length=input_length)\n",
    "  model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy', f1_m])\n",
    "  model.fit(X, y, epochs = epochs, batch_size = batch_size, verbose = 0)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6G-NyuSQQFW4"
   },
   "source": [
    "# Results and Discussion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lrA9d8piI59M"
   },
   "source": [
    "### Task B: Results and discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rwUMw69rhMXw"
   },
   "source": [
    "| Model | Epochs | Batch size | Neurons per HiddenLayer(s) | F1 score | Kaggle score |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "| Random Forest | N/A | N/A | N/A | 0.89 | 0.92 |\n",
    "| 4-layer DNN | 10 | 500 | 100 | 0.95 | 0.91 |\n",
    "| LSTM | 10 | 500 | N/A | 0.94 | 0.90 |\n",
    "| CNN | 10 | 400 | 10 | 0.95 | 0.91 |\n",
    "\n",
    "*With title*\n",
    "\n",
    "NN architectures deployed performed similarly to the baseline method. The 'title' feature captured a lot of the information relating to each document's category, illustrated by correctly classifying ~92% of test instances, alone. This makes it a well-suited predictor. Non-discernible performance differences between both shallow and deep methods deployed suggests exploration of other features may have improved performance, and allowed NNs to flourish. Moreover, alternative word-embedding methods may have been taken, which may have learned better representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Qh9x2K_RW_P"
   },
   "source": [
    "# References\n",
    "\n",
    "- Géron, A. (2019). *Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow*. 2nd ed. Sebastopol: O'Reilly.\n",
    "- Cheng, H. et al. (2016). *Wide & Deep Learning for Recommender Systems*. Google, Inc.\n",
    "- Hochreiter, S. & Schmidhuber, J. (1997). *Long Short-Term Memory*. Neural Computation, 9, pp. 1735-1780.\n",
    "- Kalchbrenner, N. et al. (2016). *A Convolutional Neural Network for Modelling Sentences*. Association for Computational Linguistics, p. 655–665.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DXtZ3k88RZ9o"
   },
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MXRs6Sn-VbeA",
    "outputId": "74ec2a4b-e59f-418e-cb1b-683cbef54845"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# If loading files via Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "# Load files\n",
    "trainMulti = pd.read_csv('gdrive/My Drive/MSc Data Analytics/CS987 assignment/german-contracts-train.csv',  dtype={\n",
    "        \"docid\":str, \"publication_date\":str, \"contract_type\":str, \"nature_of_contract\":str, \"country_code\":str,\n",
    "        \"country_name\":str, \"sector\":str, \"category\":str, \"value\":float, \"title\":str,\n",
    "        \"description\":str, \"awarding_authority\":str, \"complete_entry\":str,\n",
    "        \"label\":str   \n",
    "    })\n",
    "testMulti = pd.read_csv('gdrive/My Drive/MSc Data Analytics/CS987 assignment/german-contracts-test.csv',  dtype={\n",
    "        \"docid\":str, \"publication_date\":str, \"contract_type\":str, \"nature_of_contract\":str, \"country_code\":str,\n",
    "        \"country_name\":str, \"sector\":str, \"category\":str, \"value\":float, \"title\":str,\n",
    "        \"description\":str, \"awarding_authority\":str, \"complete_entry\":str,\n",
    "        \"label\":str   \n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7iIQT4NlDi5Z"
   },
   "outputs": [],
   "source": [
    "# If loading files via local PC\n",
    "from google.colab import files\n",
    "files.upload()\n",
    "# Load files\n",
    "import io\n",
    "train = pd.read_csv('german-contracts-train.csv')\n",
    "test = pd.read_csv('german-contracts-test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZKraR9cBsQj7"
   },
   "source": [
    "### Feature Processing\n",
    "Preprocessing for NLP tasks consists of text cleaning (lower case-ing, removal of stopwords, punctuation and special characters), tokenization and padding to make sure we have fixed dimensions to our input. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zUAtkrfvW5q8"
   },
   "source": [
    "#### Brief exploratory analysis of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oExv0lK9XhEC",
    "outputId": "bbb41c4a-082f-49af-b890-121562a10005"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- There are 98320 rows and 13 columns in the Training dataset, and 24581 rows and 11 columns in the Test dataset.\n",
      "- In the training dataset, there are 63095 Null cells.\n",
      "- In the testing dataset, there are 15826 Null cells.\n",
      "- Unique values found in the training dataset are:\n",
      " docid                 98320\n",
      "publication_date        254\n",
      "contract_type             2\n",
      "nature_of_contract        3\n",
      "country_code              1\n",
      "country_name              1\n",
      "sector                    1\n",
      "category                307\n",
      "value                 23063\n",
      "title                 32207\n",
      "description           70652\n",
      "awarding_authority    13191\n",
      "label                   176\n",
      "dtype: int64\n",
      "\n",
      "- Unique values found in the testing dataset are:\n",
      " docid                 24581\n",
      "publication_date        250\n",
      "contract_type             2\n",
      "nature_of_contract        4\n",
      "country_code              1\n",
      "country_name              1\n",
      "sector                    1\n",
      "value                  6225\n",
      "title                 13600\n",
      "description           21365\n",
      "awarding_authority     6774\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Understanding the shape of the data\n",
    "r,c = trainMulti.shape\n",
    "r1, c1 = testMulti.shape\n",
    "print(\"- There are %s rows and %s columns in the Training dataset, and %s rows and %s columns in the Test dataset.\" % (r,c, r1, c1))\n",
    "# Identifying any null values\n",
    "print('- In the training dataset, there are %s Null cells.' % trainMulti.isna().sum().sum())\n",
    "print('- In the testing dataset, there are %s Null cells.' % testMulti.isna().sum().sum())\n",
    "# Identifying the number of unique values found across features\n",
    "print('- Unique values found in the training dataset are:\\n', trainMulti.nunique())\n",
    "print('\\n- Unique values found in the testing dataset are:\\n', testMulti.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 244
    },
    "id": "ygLuP9jQXe7s",
    "outputId": "78af3732-eafc-40d2-9275-c4a60867bbc1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docid</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>contract_type</th>\n",
       "      <th>nature_of_contract</th>\n",
       "      <th>country_code</th>\n",
       "      <th>country_name</th>\n",
       "      <th>sector</th>\n",
       "      <th>category</th>\n",
       "      <th>value</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>awarding_authority</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2493527426</td>\n",
       "      <td>2020-10-14</td>\n",
       "      <td>award</td>\n",
       "      <td>services</td>\n",
       "      <td>DE</td>\n",
       "      <td>Germany</td>\n",
       "      <td>public</td>\n",
       "      <td>['Energy &amp; Environment']</td>\n",
       "      <td>75658.0</td>\n",
       "      <td>Germany-Wilhelmshaven: Cleaning services</td>\n",
       "      <td>Unterhalts- und Glasreinigung.\\n</td>\n",
       "      <td>Staatliches Baumanagement Ems-Weser</td>\n",
       "      <td>000100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2538215982</td>\n",
       "      <td>2020-11-16</td>\n",
       "      <td>notice</td>\n",
       "      <td>services</td>\n",
       "      <td>DE</td>\n",
       "      <td>Germany</td>\n",
       "      <td>public</td>\n",
       "      <td>['Infrastructure &amp; Construction']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Germany-Dresden: Engineering-design services f...</td>\n",
       "      <td>ABS Karlsruhe-Stuttgart-Nürnberg-Leipzig/Dresd...</td>\n",
       "      <td>DB Netz AG</td>\n",
       "      <td>000001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2204943443</td>\n",
       "      <td>2020-02-13</td>\n",
       "      <td>notice</td>\n",
       "      <td>works</td>\n",
       "      <td>DE</td>\n",
       "      <td>Germany</td>\n",
       "      <td>public</td>\n",
       "      <td>['Infrastructure &amp; Construction']</td>\n",
       "      <td>470000.0</td>\n",
       "      <td>Germany-Germering: Heating, ventilation and ai...</td>\n",
       "      <td>Nach Fertigstellung des ersten Bauabschnitts e...</td>\n",
       "      <td>Große Kreisstadt Germering</td>\n",
       "      <td>000001000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        docid publication_date  ...                   awarding_authority      label\n",
       "0  2493527426       2020-10-14  ...  Staatliches Baumanagement Ems-Weser  000100000\n",
       "1  2538215982       2020-11-16  ...                           DB Netz AG  000001000\n",
       "2  2204943443       2020-02-13  ...           Große Kreisstadt Germering  000001000\n",
       "\n",
       "[3 rows x 13 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Understanding the characteristics of the training data\n",
    "trainMulti.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "id": "WlCmdZE0Xewb",
    "outputId": "f986aacb-cf8d-440f-a34f-e47bab4c8b87"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docid</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>contract_type</th>\n",
       "      <th>nature_of_contract</th>\n",
       "      <th>country_code</th>\n",
       "      <th>country_name</th>\n",
       "      <th>sector</th>\n",
       "      <th>value</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>awarding_authority</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2535443526</td>\n",
       "      <td>2020-11-13</td>\n",
       "      <td>notice</td>\n",
       "      <td>services</td>\n",
       "      <td>DE</td>\n",
       "      <td>Germany</td>\n",
       "      <td>public</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Germany-Stuttgart: Software-related services</td>\n",
       "      <td>Pflege und Anpassung SKoKa-BW (1.1.2021-31.12....</td>\n",
       "      <td>Regierungspräsidium Tübingen, Abteilung 9 — La...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2487195007</td>\n",
       "      <td>2020-10-09</td>\n",
       "      <td>notice</td>\n",
       "      <td>services</td>\n",
       "      <td>DE</td>\n",
       "      <td>Germany</td>\n",
       "      <td>public</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Germany-Mühlacker: Architectural, construction...</td>\n",
       "      <td>Vergabeverfahren der Stadt Mühlacker zur Verga...</td>\n",
       "      <td>Stadtverwaltung Mühlacker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2573583192</td>\n",
       "      <td>2020-12-11</td>\n",
       "      <td>notice</td>\n",
       "      <td>supplies</td>\n",
       "      <td>DE</td>\n",
       "      <td>Germany</td>\n",
       "      <td>public</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Germany-Darmstadt: Integrated circuit packages</td>\n",
       "      <td>Für die Gruppe HEL der Abteilung ACO werden fü...</td>\n",
       "      <td>GSI Helmholtzzentrum für Schwerionenforschung ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        docid  ...                                 awarding_authority\n",
       "0  2535443526  ...  Regierungspräsidium Tübingen, Abteilung 9 — La...\n",
       "1  2487195007  ...                          Stadtverwaltung Mühlacker\n",
       "2  2573583192  ...  GSI Helmholtzzentrum für Schwerionenforschung ...\n",
       "\n",
       "[3 rows x 11 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Understanding the characteristics of the testing data\n",
    "testMulti.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rGCHwh0BXisb"
   },
   "source": [
    "**EDA informs feature processing**. \n",
    "- Many features have no unique or too many unique features: 'docid', 'country_code', 'country_name', 'sector'.\n",
    "- 'title' appears to contain similar information to 'category' / 'label'. If the text can be extracted, then this is likely the best bet to predicting classes.\n",
    "- 'description' also appears to have interesting information.\n",
    "\n",
    "The most relevant elements of this dataset are the text features: description and title. For the standard baseline model and the dense, deep neural net we will use the title feature. \n",
    "\n",
    "For the more advanced models we will use the description feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "esBJjxm2mpKJ"
   },
   "source": [
    "#### Feature preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_sktg3_Tb8ZS"
   },
   "source": [
    "**Text processing 'title'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "id": "UHUAsfwNsNoi"
   },
   "outputs": [],
   "source": [
    "# Extracting X, and removing geographic element, simultaneously\n",
    "# Train\n",
    "title_train = pd.DataFrame(trainMulti.title.str.split(':', 1).tolist(),columns = ['Location', 'title'])\n",
    "title_train.drop(columns = 'Location', inplace = True)\n",
    "# Test\n",
    "title_test = pd.DataFrame(testMulti.title.str.split(':', 1).tolist(),columns = ['Location', 'title'])\n",
    "title_test.drop(columns = 'Location', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "id": "h2r_HPkPNT22"
   },
   "outputs": [],
   "source": [
    "# Preparing text corpora (set of texts) for Tokenisation\n",
    "title_train = title_train['title']\n",
    "title_test = title_test['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "id": "P4DPo72VNYkW"
   },
   "outputs": [],
   "source": [
    "# Set up Tokenizer\n",
    "from keras_preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(char_level = False, oov_token = True, lower = True,                                               \n",
    "                      filters = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n1234567890') \n",
    "# oov_token: Any new words found in title_test will receive a new value\n",
    "# lower: All words are made lowercase, avoiding duplicates due to capitalisation\n",
    "# filters: Removes characters, numbers, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "id": "ePa1BMpuNcDM"
   },
   "outputs": [],
   "source": [
    "# Encoding 'title' using Tokenizer\n",
    "tokenizer.fit_on_texts(title_train)\n",
    "title_train = tokenizer.texts_to_sequences(title_train)\n",
    "title_test = tokenizer.texts_to_sequences(title_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "id": "3Sl3YnsaNctd"
   },
   "outputs": [],
   "source": [
    "# Pad sentences, so all 'titles' (inputs) are of fixed size\n",
    "# Ensure that 'titles' is padded to the longest sentence in title_train / title_test\n",
    "maxlen_title = max(max([len(i) for i in title_train]), max([len(i) for i in title_test]))\n",
    "title_train = pad_sequences(title_train, padding='post', maxlen = maxlen_title) # Train\n",
    "title_test = pad_sequences(title_test, padding = 'post', maxlen = maxlen_title) # Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PNACZaz-Nfle",
    "outputId": "1790caa0-7a43-42ad-e2d6-7592a97dfb03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The amount of vocabulary found by the Tokenizer in the train data is 2638\n",
      "The amount of vocabulary found by the Tokenizer in the test data is 1850\n"
     ]
    }
   ],
   "source": [
    "train_vocab_title = len(np.unique(title_train))\n",
    "print('The amount of vocabulary found by the Tokenizer in the train data is', train_vocab_title)\n",
    "test_vocab_title = len(np.unique(title_test))\n",
    "print('The amount of vocabulary found by the Tokenizer in the test data is', test_vocab_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9OveS79murw"
   },
   "source": [
    "**Text preprocessing 'description'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pQHfVS6op38y",
    "outputId": "b1cc8acd-d5ff-4ac8-be3a-1b8f2fa01819"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 152,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'stopwords' = useful German filler words\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('german')\n",
    "# Alternative Tokenizing method\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "id": "__M4ysScmzwx"
   },
   "outputs": [],
   "source": [
    "# Remove German 'stopwords' and any grammar prior to splitting\n",
    "def StopwordRemover(text):\n",
    "  text = text.str.replace(\"'\",'').str.replace(\",\",\"\").str.replace(\".\",\"\").str.replace(\"/\",\"\").str.replace(\"-\",\" \") # Punctuation found in strings\n",
    "  text = text.str.lower().str.split().apply(lambda x: [item for item in x if item not in stop]) # Lowercase, split, & remove stopwords\n",
    "  return (text)\n",
    "trainMulti['description'] = StopwordRemover(trainMulti['description'])\n",
    "testMulti['description'] = StopwordRemover(testMulti['description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "id": "Aqe6-buPmzGb"
   },
   "outputs": [],
   "source": [
    "# Preparing text corpora (set of texts) for Tokenisation\n",
    "description_train = trainMulti['description']\n",
    "description_test = testMulti['description']\n",
    "# Apply Tokenizer\n",
    "tokenizer.fit_on_texts(description_train)\n",
    "description_train = tokenizer.texts_to_sequences(description_train)\n",
    "description_test = tokenizer.texts_to_sequences(description_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "id": "J2805MFxnLei"
   },
   "outputs": [],
   "source": [
    "# Pad sentences, so all 'descriptions' (inputs) are of fixed size\n",
    "# Ensure that 'titles' is padded to the longest sentence in title_train / title_test\n",
    "maxlen_title = max(max([len(i) for i in description_train]), max([len(i) for i in description_test]))\n",
    "description_train = pad_sequences(description_train, padding='post', maxlen = maxlen_title) # Train\n",
    "description_test = pad_sequences(description_test, padding = 'post', maxlen = maxlen_title) # Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m43xuqQfnLK-",
    "outputId": "ad512021-8472-4d89-f3da-e680e192fde7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The amount of vocabulary found by the Tokenizer in the train data is 188612\n",
      "The amount of vocabulary found by the Tokenizer in the test data is 73578\n"
     ]
    }
   ],
   "source": [
    "train_vocab_description = len(np.unique(description_train))\n",
    "print('The amount of vocabulary found by the Tokenizer in the train data is', train_vocab_description)\n",
    "test_vocab_description = len(np.unique(description_test))\n",
    "print('The amount of vocabulary found by the Tokenizer in the test data is', test_vocab_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L20eELNVnA71"
   },
   "source": [
    "**One-Hot Encoding 'label' (y)**\n",
    "\n",
    "This approach is preferred over label encoding, since it reduces predictions down to binary classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "id": "eIMD1vG9NixQ"
   },
   "outputs": [],
   "source": [
    "y_train = trainMulti.copy()\n",
    "# Loop which assigns each binary label to a new column\n",
    "for i in range(len(y_train['label'][0])):\n",
    "  y_train[i] = y_train['label'].apply(lambda x: x[i])\n",
    "# Subsetting only newly created columns, and converting to integers (required by the NN)\n",
    "y_train = y_train[list(range(9))].astype('int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o-t-C8eC_09U"
   },
   "source": [
    "**Separating into training and validation data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "id": "v0_CcmWCOsvj"
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(pd.concat([pd.DataFrame(title_train), pd.DataFrame(description_train)], axis = 1), y_train, test_size = 0.3, random_state = 42)\n",
    "# Separate out title and description again\n",
    "X_train_title = np.array(X_train)[:, :25] # Title\n",
    "X_valid_title = np.array(X_valid)[:, :25]\n",
    "X_train_description = np.array(X_train)[:, 25:] # Description\n",
    "X_valid_description = np.array(X_valid)[:, 25:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EEp6DVBIOGPD"
   },
   "source": [
    "### Standard baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c8lmdzLBrEoM",
    "outputId": "b230ed72-acaf-4b32-a346-fc8d4e2781e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The F1 score of RF on title data: 0.8897913251713283\n",
      "The F1 score of RF on description data: 0.6214208203646809\n"
     ]
    }
   ],
   "source": [
    "# Shallow baseline: Random Forest \n",
    "clf = RandomForestClassifier()\n",
    "# Title\n",
    "clf.fit(X_train_title, y_train)\n",
    "RFpredictions_title = clf.predict(X_valid_title)\n",
    "print('The F1 score of RF on title data:', metrics.f1_score(y_true = y_valid, y_pred = RFpredictions_title, average = 'macro'))\n",
    "# Description\n",
    "clf.fit(X_train_description, y_train)\n",
    "RFpredictions_description = clf.predict(X_valid_description)\n",
    "print('The F1 score of RF on description data:', metrics.f1_score(y_true = y_valid, y_pred = RFpredictions_description, average = 'macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DXgHDGzxsja4"
   },
   "source": [
    "It appears that 'title' is a better predictor than 'description'. Since Random Forest can take only one input array, 'title' is chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OpkkxdF4tVWr"
   },
   "outputs": [],
   "source": [
    "# Self-developed functions to enable upload to Kaggle\n",
    "# Function: Combines each predicted column.\n",
    "def ColumnCombiner(x):\n",
    "  int_pred = np.array(x, dtype='int')\n",
    "  final_preds = []\n",
    "  for i in range(len(int_pred)):\n",
    "    string_ints = [str(int) for int in int_pred[i]]\n",
    "    str_of_ints = \"\".join(string_ints)\n",
    "    final_preds.append(str_of_ints)\n",
    "  return (final_preds)\n",
    "# Functions: Correctly formats data for submission to Kaggle.\n",
    "def KaggleUploader_Multi(x):\n",
    "  # Combine predictions w/ 'docid'\n",
    "  predictions = pd.DataFrame(x, columns = ['label'], dtype = 'str')\n",
    "  docid = pd.DataFrame(testMulti['docid'])\n",
    "  # Create prediction output for Kaggle\n",
    "  predictions = pd.concat([docid, predictions], axis=1) #concatenate ID of each query with its prediction\n",
    "  predictions.to_csv('Prediction.csv', index = False)\n",
    "  from google.colab import files\n",
    "  files.download('Prediction.csv')\n",
    "# Functions: For neural networks, converts sigmoid predictions to (0,1)\n",
    "def PredictionConverter_B(model, test_data):\n",
    "  predictions = model.predict(test_data)\n",
    "  # Transforming predictions to (0,1)\n",
    "  for i in range(len(predictions)):\n",
    "    for x in range(9):\n",
    "      if predictions[i][x] > 0.5:\n",
    "        predictions[i][x] = 1\n",
    "      else:\n",
    "        predictions[i][x] = 0\n",
    "  return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "JCrBMpdSs4y2",
    "outputId": "2afc4628-977a-4cb5-8898-c3df412f5be8"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_f9011387-fc23-4453-9855-ab9146a54481\", \"Prediction.csv\", 516213)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaggle prediction for title data: 0.91654\n"
     ]
    }
   ],
   "source": [
    "clf.fit(X_train_title, y_train_title)\n",
    "predictions = clf.predict(title_test)\n",
    "predictions = KaggleUploader_Multi(ColumnCombiner(predictions))\n",
    "print('Kaggle prediction for title data: 0.91654')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E-uM49tdsTd8"
   },
   "source": [
    "### Dense, deep neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VC_zAv3r0xKI",
    "outputId": "5851009e-f656-4e3d-a839-532cb9beca48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best F1 score: 0.9419388771057129\n",
      "Best Hyperparameters {'batch_size': 400}\n"
     ]
    }
   ],
   "source": [
    "# Use scikit-learn to grid search neurons, epochs, and batch size\n",
    "# Function to create model, required for Keras classifier\n",
    "def DenseLayerConfig_Multi():\n",
    "  model = keras.models.Sequential()\n",
    "  model.add(layers.Embedding(input_dim = train_vocab_description + 1, output_dim = 264, input_length = title_train.shape[1]))\n",
    "  model.add(layers.Flatten())\n",
    "  model.add(layers.Dense(units = 100))\n",
    "  model.add(layers.Dense(units = 50))\n",
    "  model.add(layers.Dense(units = 9, activation = 'sigmoid'))\n",
    "  # compile model\n",
    "  model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "  return model\n",
    "# random seed for reproducability\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "# Create model for GridSearch\n",
    "model = KerasClassifier(build_fn = DenseLayerConfig_Multi, epochs = 10, verbose = 1)\n",
    "# Define GridSearch Parameters\n",
    "#epochs = [10,15]\n",
    "batch_size = [400,500]\n",
    "param_grid = dict(batch_size = batch_size) # dictionary of parameters we wish to GridSearch through\n",
    "grid = GridSearchCV(estimator = model, param_grid = param_grid, n_jobs = -1, cv = 3)\n",
    "# Best parameters\n",
    "clf_title_DL = grid.fit(X_train_title, y_train, verbose = 0)\n",
    "print('Best F1 score:', clf_title_DL.best_score_)\n",
    "print('Best Hyperparameters', clf_title_DL.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3LFVQ_ArfAxB",
    "outputId": "c7f955c2-5f35-48f8-8bdb-3438fea75b63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "922/922 [==============================] - 2s 2ms/step - loss: 0.0480 - accuracy: 0.9432 - f1_m: 0.9492\n",
      "Using title data, the F1 score on the validation set is 0.9492183923721313\n"
     ]
    }
   ],
   "source": [
    "# Predictions using Title data\n",
    "clf_title_DL = ModelTrainerMulti(X_train_title, y_train, batch_size = 400, epochs = 10, model = Model_Multiclass_Dense,\n",
    "                                   input_dim = train_vocab_description +1, input_length = X_train_description.shape[1])\n",
    "loss, accuracy, f1_score = clf_title_DL.evaluate(X_valid_title, y_valid)\n",
    "print('Using title data, the F1 score on the validation set is', f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "1g6jvvGhzbu9",
    "outputId": "34cf1244-dba0-45e5-c4ed-d3462417d9e0"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_ae86c6e6-3be9-4369-9a8e-d7532329d1af\", \"Prediction.csv\", 516213)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaggle prediction for title data: 0.91268\n"
     ]
    }
   ],
   "source": [
    "# Upload Deep NN predictions\n",
    "KaggleUploader_Multi(ColumnCombiner(PredictionConverter_B(clf_title_DL, title_test)))\n",
    "print('Kaggle prediction for title data: 0.91268')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "No6pxWCIIlEh"
   },
   "source": [
    "### Additional 1: Recurrent Neural Network (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SGlTmNQdYScF",
    "outputId": "e6bb0efa-f5c6-4c9d-8309-012f224c587b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "922/922 [==============================] - 13s 13ms/step - loss: 0.0606 - accuracy: 0.9303 - f1_m: 0.9360\n",
      "Using title data, the F1 score on the validation set is 0.9359655976295471\n"
     ]
    }
   ],
   "source": [
    "# Predictions\n",
    "clf_title_RNN = ModelTrainerMulti(X_train_title, y_train, batch_size = 500, epochs = 10, model = Model_Multiclass_LSTM, \n",
    "                                  input_dim = train_vocab_title +1, input_length = X_train_title.shape[1])\n",
    "loss, accuracy, f1_score = clf_title_RNN.evaluate(X_valid_title, y_valid)\n",
    "print('Using title data, the F1 score on the validation set is', f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "4z9gURq5b_H6",
    "outputId": "5c3f7878-b09e-4e95-c180-056a8b0e361f"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_dff1311b-6841-4c44-bb2e-35e3364dafa6\", \"Prediction.csv\", 516213)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaggle prediction for title data: 0.90210\n"
     ]
    }
   ],
   "source": [
    "# Upload LSTM predictions\n",
    "KaggleUploader_Multi(ColumnCombiner(PredictionConverter_B(clf_title_RNN, title_test)))\n",
    "print('Kaggle prediction for title data: 0.90210')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y54_l1t5Irox"
   },
   "source": [
    "### Additional 2: Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wcniBH_nYxye",
    "outputId": "7cedbffc-9ac3-414e-a990-54c635cc1545"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "922/922 [==============================] - 2s 2ms/step - loss: 0.0558 - accuracy: 0.9404 - f1_m: 0.9457\n",
      "Using title data, the F1 score on the validation set is 0.9457349181175232\n"
     ]
    }
   ],
   "source": [
    "# Prediction\n",
    "clf_title_CNN = ModelTrainerMulti(X_train_title, y_train, batch_size = 400, epochs = 10, model = Model_Multiclass_CNN, \n",
    "                                  input_dim = train_vocab_title +1, input_length = X_train_title.shape[1])\n",
    "loss, accuracy, f1_score = clf_title_CNN.evaluate(X_valid_title, y_valid)\n",
    "print('Using title data, the F1 score on the validation set is', f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "F8aOD3Y5dfcq",
    "outputId": "cee09ac2-b5f5-4a0c-fcda-6e15c0ec0047"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_f42b690c-d5fa-4883-9ae7-81e740ee967c\", \"Prediction.csv\", 516213)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaggle prediction for title data: 0.91232\n"
     ]
    }
   ],
   "source": [
    "# Upload CNN predictions\n",
    "KaggleUploader_Multi(ColumnCombiner(PredictionConverter_B(clf_title_CNN, title_test)))\n",
    "print('Kaggle prediction for title data: 0.91232')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [
    "wJPrzHAyex1s"
   ],
   "machine_shape": "hm",
   "name": "CS985 / CS987 neural networks project.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
